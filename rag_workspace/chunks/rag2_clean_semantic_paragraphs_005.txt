Avg. # words

Propositions

Table 1: Statistics of text units in the English Wikipedia.
the original text, e.g. the reference of the tower is
resolved into its full mention, the Leaning Tower
of Pisa, in the first proposition. We expect each
proposition to describe exactly one atomic fact, and
so our intuition is that propositions would suitably
work as a retrieval unit for information-seeking

FACTOIDWIKI: Proposition-Level
Index and Retrieval for Wikipedia
We empirically compare passages, sentences, and
propositions as retrieval units on Wikipedia, a
commonly-used retrieval source for knowledge-
intensive NLP tasks (Petroni et al., 2021). To allow
a fair comparison across granularities, we process
an English Wikipedia dump from 2021-10-13, as
used by Bohnet et al. (2022). We segment each doc-
ument text into three different granularities: pas-
sages, sentences, and propositions. We include the
details on passage- and sentence-level segmenta-
tion of the corpus in Appendix A.
Parsing Passage to Propositions.

the Wikipedia pages into propositions, we finetune
a text generation model, which we refer to as the
Propositionizer. The Propositionizer takes a pas-
sage as input and generates the list of propositions
within the passage.
Following Chen et al. (2023b), we train the
Propositionizer with a two-step distillation process.
We first prompt GPT-4 (OpenAI, 2023) with an
instruction containing the proposition definition
and 1-shot demonstration. We include the details
of the prompt in Figure 8. We start with a set of
42k passages and use GPT-4 to generate the seed
set of paragraph-to-proposition pairs. Next, we
use the seed set to finetune a Flan-T5-large model
(Chung et al., 2022). We refer to the processed
corpus as FACTOIDWIKI. The statistics of FAC-
TOIDWIKI are shown in Table 1.
Quality Analysis.
We conduct a manual error
analysis to understand the quality of propositions
generated by GPT-4 and the Propositionizer. While
there does not exist a fixed standard on deciding
--- Page 4 ---

Propositionizer
Not Faithful
0.7% (3/408)
1.3% (6/445)
Not Minimal
2.9% (12/408)
2.0% (9/445)
Not Stand-alone
4.9% (20/408)
3.1% (14/445)
Table 2: Frequency of errors occurred in the generated
propositions. Most generated propositions are faithful,
while a small portion of them are not stand-alone.
a ground truth set of propositions for a passage,
we estimate the frequency of error cases where
(1) a proposition is not fully supported by the pas-
sage, (2) a proposition can be further split into
separate propositions, and (3) propositions are not
self-contained, respectively (Table 2). On a random
sample of 50 passages, we observe that almost all
propositions generated by both models are faithful,
while a small portion of the propositions are not
stand-alone.
Experimental Settings
To evaluate the impact of the three retrieval unit
choices, we conduct experiments on five differ-
ent open-domain QA datasets with FACTOIDWIKI.
With each dataset, we evaluate both passage re-
trieval and downstream QA performance when
dense retrievers work with Wikipedia indexed in
different granularities.